{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# TPU vs GPU Inference Benchmark\n\n**Purpose**: Compare inference cost/performance across hardware for investment thesis.\n\n**Hardware Options** (Colab Dec 2024):\n- **TPU**: v6e-1, v5e-1\n- **GPU**: L4, T4, A100 (high VRAM switch available)\n\n**Models Tested**:\n- Embedding: Qwen3-Embedding-4B\n- Reranking: Qwen3-Reranker-4B\n- Inference: Mistral-7B, Llama-3-8B",
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup - run this first\n",
    "!pip install -q torch transformers accelerate sentencepiece\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Detect hardware\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    hw_name = torch.cuda.get_device_name(0)\n",
    "    hw_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {hw_name} ({hw_mem:.1f}GB)\")\n",
    "elif 'TPU_NAME' in os.environ:\n",
    "    device = \"xla\"\n",
    "    hw_name = os.environ.get('TPU_NAME', 'TPU')\n",
    "    hw_mem = 'N/A'\n",
    "    print(f\"TPU: {hw_name}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    hw_name = \"CPU\"\n",
    "    hw_mem = 'N/A'\n",
    "    print(\"Warning: No GPU/TPU detected\")"
   ],
   "metadata": {
    "id": "setup"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Benchmark: Embedding throughput\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_ID = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"  # Smaller for free tier\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Test data\n",
    "texts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do neural networks learn?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "] * 100  # 300 texts\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(texts[:10], return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    _ = model(**inputs)\n",
    "\n",
    "# Benchmark\n",
    "batch_size = 32\n",
    "start = time.time()\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        total_tokens += inputs['input_ids'].numel()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "tokens_per_sec = total_tokens / elapsed\n",
    "\n",
    "result = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"hardware\": hw_name,\n",
    "    \"hardware_mem_gb\": hw_mem,\n",
    "    \"model\": MODEL_ID,\n",
    "    \"task\": \"embedding\",\n",
    "    \"texts_processed\": len(texts),\n",
    "    \"total_tokens\": total_tokens,\n",
    "    \"elapsed_seconds\": round(elapsed, 2),\n",
    "    \"tokens_per_second\": round(tokens_per_sec, 1),\n",
    "    \"texts_per_second\": round(len(texts) / elapsed, 1),\n",
    "}\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ],
   "metadata": {
    "id": "embedding_benchmark"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save results for aggregation\n",
    "import os\n",
    "\n",
    "results_dir = \"/content/benchmark_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "filename = f\"{results_dir}/{hw_name.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"Saved to {filename}\")\n",
    "print(\"\\nTo download: Files -> benchmark_results/\")"
   ],
   "metadata": {
    "id": "save_results"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Investment Thesis Data Points\n\nAfter running on different hardware tiers, compare:\n\n| Hardware | Type | VRAM | Cost/hr | Tokens/sec | Cost per 1M tokens |\n|----------|------|------|---------|------------|--------------------|\n| T4 | GPU | 16GB | $0 (free) | ? | ? |\n| L4 | GPU | 24GB | ? | ? | ? |\n| A100 | GPU | 40/80GB | ? | ? | ? |\n| v5e-1 | TPU | - | ? | ? | ? |\n| v6e-1 | TPU | - | ? | ? | ? |\n\n**Key questions**:\n1. At what scale does TPU beat GPU?\n2. What's the break-even vs API pricing (OpenRouter ~$0.10/1M tokens)?\n3. Which workloads favor which hardware?\n4. L4 vs T4 - worth the upgrade?",
   "metadata": {
    "id": "thesis"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Cross-Hardware Matrix (The Complete Picture)\n\nCurrent notebook measures **same-hardware** inference. But real investment thesis needs:\n\n|  | **Serve GPU** | **Serve TPU** |\n|--|---------------|---------------|\n| **Train GPU** | Baseline (PyTorch) | Export to JAX |\n| **Train TPU** | Export to ONNX | Native JAX/Flax |\n\n**Next notebooks:**\n- `cross_hardware_serving.ipynb` - conversion overhead\n- `training_cost.ipynb` - train time comparison  \n- `total_cost_calculator.ipynb` - optimal split for N requests",
   "metadata": {}
  }
 ]
}