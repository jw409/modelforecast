{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jw409/modelforecast/blob/main/notebooks/colab_tpu_gpu_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TPU vs GPU Inference Benchmark\n",
    "\n",
    "**Purpose**: Compare inference cost/performance across hardware for investment thesis.\n",
    "\n",
    "**Hardware Options** (Colab Dec 2024):\n",
    "- **TPU**: v6e-1, v5e-1\n",
    "- **GPU**: L4, T4, A100 (high VRAM switch available)\n",
    "\n",
    "**Models Tested**:\n",
    "- Embedding: Qwen3-Embedding-4B\n",
    "- Reranking: Qwen3-Reranker-4B\n",
    "- Inference: Mistral-7B, Llama-3-8B"
   ],
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "code",
   "source": "# Setup - run this first\n!pip install -q torch transformers accelerate sentencepiece\n!pip install -q huggingface_hub\n\nimport os\nimport torch\nimport time\nimport json\nfrom datetime import datetime\n\n# Detect hardware\nif torch.cuda.is_available():\n    device = \"cuda\"\n    hw_name = torch.cuda.get_device_name(0)\n    hw_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {hw_name} ({hw_mem:.1f}GB)\")\nelif 'TPU_NAME' in os.environ:\n    device = \"xla\"\n    hw_name = os.environ.get('TPU_NAME', 'TPU')\n    hw_mem = 'N/A'\n    print(f\"TPU: {hw_name}\")\nelse:\n    device = \"cpu\"\n    hw_name = \"CPU\"\n    hw_mem = 'N/A'\n    print(\"Warning: No GPU/TPU detected\")",
   "metadata": {
    "id": "setup",
    "outputId": "21948493-97ae-4023-e406-b2275aaa53e0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Benchmark: Embedding throughput\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"  # 80MB, fast download\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModel.from_pretrained(MODEL_ID).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Test data\n",
    "texts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do neural networks learn?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "] * 100  # 300 texts\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(texts[:10], return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    _ = model(**inputs)\n",
    "\n",
    "# Benchmark\n",
    "batch_size = 32\n",
    "start = time.time()\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        total_tokens += inputs['input_ids'].numel()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "tokens_per_sec = total_tokens / elapsed\n",
    "\n",
    "result = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"hardware\": hw_name,\n",
    "    \"hardware_mem_gb\": hw_mem,\n",
    "    \"model\": MODEL_ID,\n",
    "    \"task\": \"embedding\",\n",
    "    \"texts_processed\": len(texts),\n",
    "    \"total_tokens\": total_tokens,\n",
    "    \"elapsed_seconds\": round(elapsed, 2),\n",
    "    \"tokens_per_second\": round(tokens_per_sec, 1),\n",
    "    \"texts_per_second\": round(len(texts) / elapsed, 1),\n",
    "}\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ],
   "metadata": {
    "id": "embedding_benchmark",
    "outputId": "f9cee1b8-07d6-45d7-97a9-cb432b137adf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488,
     "referenced_widgets": [
      "d331db22b1384ad2b9ec7f97abf00648",
      "77b762aff15740d0b50aea56930cea85",
      "a7d5a308e2ce4e749862eac9524d8bd1",
      "68cacf1ad9f94d39878aec54230ed74b",
      "fdaf8a570f9c421ca8a9a78ad1d12350",
      "19b82ddb16f24cabb9289b92ffc2790a",
      "bb8118d268d74b6fadd80244e25df419",
      "9571c361d9fc4d30a5ed7479be3ad123",
      "e9659081273c4fc695d0a1b757c7a96f",
      "7b69e65ce1c34d7084a554db7565040d",
      "455458dcb31e41ddb36c09a36f1f08a5",
      "d93d527faeba4b7b8272086094d7b454",
      "c01f6edadde14f3bb376911b6995752f"
     ]
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Alibaba-NLP/gte-Qwen2-1.5B-instruct...\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d331db22b1384ad2b9ec7f97abf00648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b762aff15740d0b50aea56930cea85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_qwen.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct:\n",
      "- tokenization_qwen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d5a308e2ce4e749862eac9524d8bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cacf1ad9f94d39878aec54230ed74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdaf8a570f9c421ca8a9a78ad1d12350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b82ddb16f24cabb9289b92ffc2790a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8118d268d74b6fadd80244e25df419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/370 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9571c361d9fc4d30a5ed7479be3ad123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/901 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9659081273c4fc695d0a1b757c7a96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_qwen.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct:\n",
      "- modeling_qwen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b69e65ce1c34d7084a554db7565040d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455458dcb31e41ddb36c09a36f1f08a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93d527faeba4b7b8272086094d7b454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01f6edadde14f3bb376911b6995752f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save results for aggregation\n",
    "import os\n",
    "\n",
    "results_dir = \"/content/benchmark_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "filename = f\"{results_dir}/{hw_name.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"Saved to {filename}\")\n",
    "print(\"\\nTo download: Files -> benchmark_results/\")"
   ],
   "metadata": {
    "id": "save_results"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Investment Thesis Data Points\n",
    "\n",
    "After running on different hardware tiers, compare:\n",
    "\n",
    "| Hardware | Type | VRAM | Cost/hr | Tokens/sec | Cost per 1M tokens |\n",
    "|----------|------|------|---------|------------|--------------------|\n",
    "| T4 | GPU | 16GB | $0 (free) | ? | ? |\n",
    "| L4 | GPU | 24GB | ? | ? | ? |\n",
    "| A100 | GPU | 40/80GB | ? | ? | ? |\n",
    "| v5e-1 | TPU | - | ? | ? | ? |\n",
    "| v6e-1 | TPU | - | ? | ? | ? |\n",
    "\n",
    "**Key questions**:\n",
    "1. At what scale does TPU beat GPU?\n",
    "2. What's the break-even vs API pricing (OpenRouter ~$0.10/1M tokens)?\n",
    "3. Which workloads favor which hardware?\n",
    "4. L4 vs T4 - worth the upgrade?"
   ],
   "metadata": {
    "id": "thesis"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-Hardware Matrix (The Complete Picture)\n",
    "\n",
    "Current notebook measures **same-hardware** inference. But real investment thesis needs:\n",
    "\n",
    "|  | **Serve GPU** | **Serve TPU** |\n",
    "|--|---------------|---------------|\n",
    "| **Train GPU** | Baseline (PyTorch) | Export to JAX |\n",
    "| **Train TPU** | Export to ONNX | Native JAX/Flax |\n",
    "\n",
    "**Next notebooks:**\n",
    "- `cross_hardware_serving.ipynb` - conversion overhead\n",
    "- `training_cost.ipynb` - train time comparison  \n",
    "- `total_cost_calculator.ipynb` - optimal split for N requests"
   ],
   "metadata": {
    "id": "Wk2Cz1Q2Tqu4"
   }
  }
 ]
}