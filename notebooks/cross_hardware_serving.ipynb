{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Hardware Serving Benchmark\n",
        "\n",
        "**Question**: What's the overhead of training on one hardware and serving on another?\n",
        "\n",
        "## The Matrix\n",
        "\n",
        "|  | **Serve GPU** | **Serve TPU** |\n",
        "|--|---------------|---------------|\n",
        "| **Train GPU** | Baseline | PyTorch→JAX |\n",
        "| **Train TPU** | JAX→ONNX→TensorRT | Native |\n",
        "\n",
        "## Conversion Paths\n",
        "\n",
        "1. **PyTorch → JAX** (GPU train → TPU serve)\n",
        "   - `torch` → `jax.numpy` weight conversion\n",
        "   - Or: `torch` → ONNX → `jax_onnx`\n",
        "   - Friction: High (architecture differences)\n",
        "\n",
        "2. **JAX → ONNX → TensorRT** (TPU train → GPU serve)\n",
        "   - `jax2onnx` or `flax` export\n",
        "   - ONNX → TensorRT for GPU optimization\n",
        "   - Friction: Medium (well-supported path)\n",
        "\n",
        "3. **PyTorch → ONNX → TensorRT** (GPU train → GPU serve optimized)\n",
        "   - Standard optimization path\n",
        "   - Friction: Low (native ecosystem)"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "!pip install -q torch transformers onnx onnxruntime-gpu\n",
        "!pip install -q optimum[onnxruntime-gpu]\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Detect hardware\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    hw_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"GPU: {hw_name}\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    hw_name = \"CPU\"\n",
        "    print(\"Warning: No GPU detected\")"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load PyTorch model (simulating GPU-trained model)\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "MODEL_ID = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
        "\n",
        "print(f\"Loading {MODEL_ID} in PyTorch...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model_pt = AutoModel.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# Baseline PyTorch inference\n",
        "test_text = \"What is the meaning of life?\"\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
        "\n",
        "model_pt.eval()\n",
        "with torch.no_grad():\n",
        "    start = time.time()\n",
        "    for _ in range(10):\n",
        "        _ = model_pt(**inputs)\n",
        "    pt_time = (time.time() - start) / 10\n",
        "\n",
        "print(f\"PyTorch CPU baseline: {pt_time*1000:.1f}ms/inference\")"
      ],
      "metadata": {
        "id": "pytorch_baseline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Export to ONNX\n",
        "from pathlib import Path\n",
        "\n",
        "onnx_path = Path(\"/content/model.onnx\")\n",
        "\n",
        "print(\"Exporting to ONNX...\")\n",
        "export_start = time.time()\n",
        "\n",
        "# Dynamic axes for variable sequence length\n",
        "dynamic_axes = {\n",
        "    'input_ids': {0: 'batch', 1: 'sequence'},\n",
        "    'attention_mask': {0: 'batch', 1: 'sequence'},\n",
        "}\n",
        "\n",
        "torch.onnx.export(\n",
        "    model_pt,\n",
        "    (inputs['input_ids'], inputs['attention_mask']),\n",
        "    onnx_path,\n",
        "    input_names=['input_ids', 'attention_mask'],\n",
        "    output_names=['last_hidden_state'],\n",
        "    dynamic_axes=dynamic_axes,\n",
        "    opset_version=14,\n",
        ")\n",
        "\n",
        "export_time = time.time() - export_start\n",
        "onnx_size = onnx_path.stat().st_size / 1e9\n",
        "\n",
        "print(f\"ONNX export: {export_time:.1f}s, size: {onnx_size:.2f}GB\")"
      ],
      "metadata": {
        "id": "onnx_export"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: ONNX Runtime inference (GPU)\n",
        "import onnxruntime as ort\n",
        "\n",
        "print(\"Loading ONNX model with GPU provider...\")\n",
        "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "session = ort.InferenceSession(str(onnx_path), providers=providers)\n",
        "\n",
        "# Check which provider is being used\n",
        "print(f\"Using provider: {session.get_providers()}\")\n",
        "\n",
        "# Prepare inputs\n",
        "ort_inputs = {\n",
        "    'input_ids': inputs['input_ids'].numpy(),\n",
        "    'attention_mask': inputs['attention_mask'].numpy(),\n",
        "}\n",
        "\n",
        "# Warmup\n",
        "for _ in range(3):\n",
        "    _ = session.run(None, ort_inputs)\n",
        "\n",
        "# Benchmark\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = session.run(None, ort_inputs)\n",
        "onnx_time = (time.time() - start) / 100\n",
        "\n",
        "print(f\"ONNX Runtime GPU: {onnx_time*1000:.1f}ms/inference\")\n",
        "print(f\"Speedup vs PyTorch CPU: {pt_time/onnx_time:.1f}x\")"
      ],
      "metadata": {
        "id": "onnx_inference"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Results summary\n",
        "results = {\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"model\": MODEL_ID,\n",
        "    \"hardware\": hw_name,\n",
        "    \"pytorch_cpu_ms\": round(pt_time * 1000, 1),\n",
        "    \"onnx_export_seconds\": round(export_time, 1),\n",
        "    \"onnx_size_gb\": round(onnx_size, 2),\n",
        "    \"onnx_gpu_ms\": round(onnx_time * 1000, 1),\n",
        "    \"speedup\": round(pt_time / onnx_time, 1),\n",
        "    \"conversion_overhead\": \"one-time\",\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CROSS-HARDWARE SERVING RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(json.dumps(results, indent=2))\n",
        "\n",
        "# Investment insight\n",
        "breakeven_inferences = export_time / (pt_time - onnx_time) if pt_time > onnx_time else float('inf')\n",
        "print(f\"\\nBreak-even: {breakeven_inferences:.0f} inferences to recoup export cost\")"
      ],
      "metadata": {
        "id": "results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Investment Thesis Insights\n",
        "\n",
        "**Conversion overhead is ONE-TIME cost.**\n",
        "\n",
        "| Scenario | Export Time | Per-Inference Gain | Break-even |\n",
        "|----------|-------------|--------------------|-----------|\n",
        "| PT→ONNX→GPU | ? sec | ? ms | ? inferences |\n",
        "| PT→JAX→TPU | ? sec | ? ms | ? inferences |\n",
        "| JAX→ONNX→GPU | ? sec | ? ms | ? inferences |\n",
        "\n",
        "**Key insight**: If serving >N requests, cross-hardware conversion PAYS OFF.\n",
        "\n",
        "Fill in with your runs across T4, L4, A100, v5e, v6e."
      ],
      "metadata": {
        "id": "thesis"
      }
    }
  ]
}
